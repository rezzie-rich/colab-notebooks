{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOQhjDA4Fax81lTHR7E6jB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezzie-rich/colab-notebooks/blob/main/RB_MisLoROpt_t1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqV95Lnxyx3I"
      },
      "outputs": [],
      "source": [
        "# Modularized and Dynamic Training Script for Mistral 7B with LoRA, PEFT, and Hyperparameter Optimization\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq, trainer_utils)\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import notebook_login, HfFolder\n",
        "import optuna\n",
        "from optuna.integration import HuggingFacePruner\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "notebook_login()\n",
        "\n",
        "# Setting up logging for debugging and tracking\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Custom Callback for Checkpointing during training\n",
        "class CustomCheckpointCallback(trainer_utils.Callback):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        # Save model at every 100 steps for tracking and potential rollback\n",
        "        if state.global_step % 100 == 0:\n",
        "            self.model.save_pretrained(f'{args.output_dir}/checkpoint_{state.global_step}')\n",
        "\n",
        "# Function Definitions\n",
        "\n",
        "def input_with_validation(prompt, type_=None, validation=None, error_msg='Invalid input'):\n",
        "    \"\"\"Validates and converts user input.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            value = input(prompt)\n",
        "            if type_:\n",
        "                value = type_(value)\n",
        "            if validation and not validation(value):\n",
        "                raise ValueError\n",
        "            return value\n",
        "        except ValueError:\n",
        "            print(error_msg)\n",
        "\n",
        "def initialize_model_and_tokenizer(model_name):\n",
        "    \"\"\"Initializes the model and tokenizer.\"\"\"\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error initializing model and tokenizer: {e}\")\n",
        "        raise\n",
        "\n",
        "def configure_lora(model, r, lora_alpha, target_modules, lora_dropout, bias_config):\n",
        "    \"\"\"\n",
        "    Configures LoRA for the model. LoRA allows efficient tuning of large language models by\n",
        "    learning low-rank updates instead of full-rank weight matrices, reducing the number of trainable parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        lora_config = LoraConfig(r=r, lora_alpha=lora_alpha, target_modules=target_modules, lora_dropout=lora_dropout, bias=bias_config)\n",
        "        # Preparing model for quantized INT8 training for efficiency\n",
        "        model = prepare_model_for_int8_training(model)\n",
        "        # Applying PEFT for efficient distributed training\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error configuring LoRA: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_and_prepare_dataset(dataset_name, text_field, response_field=None, local_path=None, tokenizer=None, max_length=512):\n",
        "    \"\"\"\n",
        "    Loads and prepares the dataset. This includes loading the dataset from a local path or from Hugging Face,\n",
        "    tokenizing the text, and formatting it properly for the model training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if local_path:\n",
        "            dataset = Dataset.from_pandas(pd.read_csv(local_path))\n",
        "        else:\n",
        "            dataset = load_dataset(dataset_name, split='train')\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            if response_field:\n",
        "                text = ['Prompt: ' + ex[text_field] + '\\nResponse: ' + ex[response_field] for ex in examples]\n",
        "            else:\n",
        "                text = examples[text_field]\n",
        "            return tokenizer(text, truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "        dataset = dataset.map(preprocess_function, batched=True)\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading and preparing dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "def train_model(model, training_args, train_dataset, test_dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Trains the model with the provided datasets. Uses Trainer from Hugging Face for simplicity and efficiency.\n",
        "    Custom checkpoints are used for saving the model at regular intervals.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=-100, pad_to_multiple_of=8)\n",
        "        trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, data_collator=data_collator, callbacks=[CustomCheckpointCallback(model)])\n",
        "        trainer.train()\n",
        "        return trainer\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during model training: {e}\")\n",
        "        raise\n",
        "\n",
        "def optimize_hyperparameters(model, tokenizer, datasets_info, base_training_args):\n",
        "    def objective(trial):\n",
        "        # Define the hyperparameters to optimize\n",
        "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
        "        num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
        "        per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32])\n",
        "\n",
        "        # Update base training arguments with suggestions\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=base_training_args.output_dir,\n",
        "            learning_rate=learning_rate,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            # ... other arguments from base_training_args ...\n",
        "        )\n",
        "\n",
        "        # Run the training and evaluation loop for each dataset\n",
        "        for i, dataset_info in enumerate(datasets_info):\n",
        "            # Load and prepare dataset\n",
        "            dataset = load_and_prepare_dataset(**dataset_info, tokenizer=tokenizer)\n",
        "            train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
        "            trainer = train_model(model, training_args, train_dataset, test_dataset, tokenizer)\n",
        "\n",
        "        # Here you should return a metric from the evaluation on the test set\n",
        "        # For instance, if the trainer returns a dictionary with an 'eval_loss' you can use that\n",
        "        eval_result = trainer.evaluate()\n",
        "        return eval_result[\"eval_loss\"]  # or another metric that you prefer\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=10)  # You can specify the number of trials\n",
        "\n",
        "    # Log the best hyperparameters\n",
        "    logging.info(f\"Best trial: {study.best_trial.params}\")\n",
        "\n",
        "    # You can return the best parameters here if you want to use them for further training\n",
        "    return study.best_trial.params\n",
        "\n",
        "# Experience Replay\n",
        "def experience_replay(previous_dataset, current_dataset, replay_rate=0.1):\n",
        "    \"\"\"\n",
        "    This function takes a fraction of the previous dataset and adds it to the current training dataset.\n",
        "    It helps in preventing catastrophic forgetting by retraining the model on a portion of the previous data.\n",
        "    \"\"\"\n",
        "    replay_samples = previous_dataset.shuffle(seed=42).select(range(int(replay_rate * len(previous_dataset))))\n",
        "    combined_dataset = concatenate_datasets([current_dataset, replay_samples])\n",
        "    return combined_dataset.shuffle(seed=42)\n",
        "\n",
        "# Main Script Execution\n",
        "def main():\n",
        "     try:\n",
        "        # Model and tokenizer initialization\n",
        "        model_name = input(\"Enter the model name (e.g., 'mistralai/Mistral-7B-v0.1'): \")\n",
        "        model, tokenizer = initialize_model_and_tokenizer(model_name)\n",
        "\n",
        "        # Configuring LoRA parameters\n",
        "        # LoRA rank and alpha values should be tuned based on the specific model and training data\n",
        "        r = input_with_validation(\"Enter LoRA rank (e.g., 16): \", int)\n",
        "        lora_alpha = input_with_validation(\"Enter LoRA alpha (e.g., 32): \", int)\n",
        "        target_modules = input(\"Enter target modules separated by comma (e.g., 'q_proj,k_proj,v_proj,o_proj'): \").split(',')\n",
        "        lora_dropout = input_with_validation(\"Enter LoRA dropout rate (e.g., 0.05): \", float)\n",
        "        bias_config = input(\"Enter bias configuration ('none' or other): \")\n",
        "        model = configure_lora(model, r, lora_alpha, target_modules, lora_dropout, bias_config)\n",
        "\n",
        "        # Ask the user if they want to perform hyperparameter optimization\n",
        "        perform_optuna_optimization = input(\"Do you want to perform hyperparameter optimization? (yes/no): \").lower() == 'yes'\n",
        "\n",
        "        # Training arguments setup\n",
        "        learning_rate = input_with_validation(\"Enter learning rate (e.g., 1e-3): \", float)\n",
        "        num_train_epochs = input_with_validation(\"Enter number of training epochs (e.g., 5): \", int)\n",
        "        batch_size = input_with_validation(\"Enter per device train batch size (e.g., 8): \", int)\n",
        "        output_dir = input(\"Enter output directory path: \")\n",
        "        training_args = TrainingArguments(output_dir=output_dir, learning_rate=learning_rate, num_train_epochs=num_train_epochs, per_device_train_batch_size=batch_size, logging_steps=50, save_steps=100, fp16=True, gradient_checkpointing=True, evaluation_strategy=\"epoch\", warmup_steps=500, weight_decay=0.01, lr_scheduler_type=\"linear\")\n",
        "\n",
        "        # Prepare datasets information\n",
        "        num_datasets = input_with_validation(\"Enter the number of datasets: \", int)\n",
        "        datasets_info = []\n",
        "        for i in range(num_datasets):\n",
        "            dataset_name = input(f\"Enter the dataset name for dataset {i+1} (leave blank if using local dataset): \")\n",
        "            text_field = input(f\"Enter the name of the text field in the dataset for dataset {i+1}: \")\n",
        "            response_field = input(f\"Enter the name of the response field in the dataset for dataset {i+1} (leave blank if not applicable): \")\n",
        "            local_path = input(f\"Enter local dataset path for dataset {i+1} (leave blank if using Hugging Face dataset): \")\n",
        "            datasets_info.append({\n",
        "                'dataset_name': dataset_name,\n",
        "                'text_field': text_field,\n",
        "                'response_field': response_field,\n",
        "                'local_path': local_path\n",
        "            })\n",
        "\n",
        "        if perform_optuna_optimization:\n",
        "            # Perform hyperparameter optimization\n",
        "            best_params = optimize_hyperparameters(model, tokenizer, datasets_info, training_args)\n",
        "            logging.info(f\"Optuna optimization completed. Best parameters: {best_params}\")\n",
        "\n",
        "            # Update training_args with best_params\n",
        "            training_args.learning_rate = best_params.get('learning_rate', training_args.learning_rate)\n",
        "            training_args.num_train_epochs = best_params.get('num_train_epochs', training_args.num_train_epochs)\n",
        "            training_args.per_device_train_batch_size = best_params.get('per_device_train_batch_size', training_args.per_device_train_batch_size)\n",
        "\n",
        "        previous_dataset = None\n",
        "        for dataset_info in datasets_info:\n",
        "            dataset = load_and_prepare_dataset(**dataset_info, tokenizer=tokenizer)\n",
        "            if previous_dataset is not None:\n",
        "                dataset = experience_replay(previous_dataset, dataset)\n",
        "            previous_dataset = dataset\n",
        "\n",
        "            # Splitting the dataset into training and testing sets\n",
        "            train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
        "\n",
        "            # Starting the training process\n",
        "            trainer = train_model(model, training_args, train_dataset, test_dataset, tokenizer)\n",
        "\n",
        "        # Uploading the trained model to Hugging Face for easy access and version control\n",
        "        trainer.save_model()\n",
        "        if trainer.is_world_process_zero():\n",
        "            tokenizer.save_pretrained(training_args.output_dir)\n",
        "            model.push_to_hub(f\"{model_name}_trained_model\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred in the main script: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}