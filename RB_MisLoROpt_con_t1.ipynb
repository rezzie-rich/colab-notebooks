{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGNh6Z544nrbw1PXx7zvC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezzie-rich/colab-notebooks/blob/main/RB_MisLoROpt_con_t1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model:\n",
        "  name: 'mistral-7b-model'\n",
        "  torch_dtype: 'torch.bfloat16'\n",
        "\n",
        "lora:\n",
        "  enable: true\n",
        "  config:\n",
        "    lora_r: 4\n",
        "    lora_alpha: 32\n",
        "\n",
        "training:\n",
        "  global_args:\n",
        "    output_dir: './model_output'\n",
        "    logging_dir: './logs'\n",
        "    evaluation_strategy: 'steps'\n",
        "    logging_steps: 500\n",
        "    save_strategy: 'epoch'\n",
        "    save_total_limit: 3\n",
        "    load_best_model_at_end: true\n",
        "    metric_for_best_model: 'loss'\n",
        "    greater_is_better: false\n",
        "  datasets_info:\n",
        "    - dataset_name: 'dataset1'\n",
        "      split: 'train'\n",
        "      text_fields: ['text_field1', 'text_field2']\n",
        "      label_fields:\n",
        "        label1: 'categorical'\n",
        "        label2: 'continuous'\n",
        "      max_token_length: 512\n",
        "      lora:\n",
        "        enable: true\n",
        "        config:\n",
        "          lora_r: 4\n",
        "          lora_alpha: 32\n",
        "      training_args:\n",
        "        learning_rate: 5e-5\n",
        "        num_train_epochs: 3\n",
        "        per_device_train_batch_size: 8\n",
        "        warmup_steps: 100\n",
        "        weight_decay: 0.01\n",
        "      optuna:\n",
        "        enable: true\n",
        "        study_name: 'optuna_study_dataset1'\n",
        "        direction: 'minimize'\n",
        "        n_trials: 100\n",
        "        param_ranges:\n",
        "          learning_rate:\n",
        "            low: 1e-5\n",
        "            high: 1e-4\n",
        "          num_train_epochs:\n",
        "            low: 2\n",
        "            high: 5\n",
        "          per_device_train_batch_size:\n",
        "            options: [8, 16, 32]\n",
        "\n",
        "    - dataset_name: 'dataset2'\n",
        "      split: 'test'\n",
        "      text_fields: ['content']\n",
        "      label_fields:\n",
        "        category: 'categorical'\n",
        "      max_token_length: 256\n",
        "      lora:\n",
        "        enable: true\n",
        "        config:\n",
        "          lora_r: 8\n",
        "          lora_alpha: 16\n",
        "      training_args:\n",
        "        learning_rate: 3e-5\n",
        "        num_train_epochs: 5\n",
        "        per_device_train_batch_size: 16\n",
        "        warmup_steps: 50\n",
        "        weight_decay: 0.02\n",
        "      optuna:\n",
        "        enable: false\n",
        "\n",
        "    - dataset_name: 'dataset3'\n",
        "      split: 'validation'\n",
        "      text_fields: ['description']\n",
        "      label_fields:\n",
        "        score: 'continuous'\n",
        "      max_token_length: 128\n",
        "      lora:\n",
        "        enable: false\n",
        "      training_args:\n",
        "        learning_rate: 2e-5\n",
        "        num_train_epochs: 4\n",
        "        per_device_train_batch_size: 32\n",
        "        warmup_steps: 75\n",
        "        weight_decay: 0.03\n",
        "      optuna:\n",
        "        enable: false\n",
        "\n",
        "experience_replay:\n",
        "  enable: true\n",
        "  replay_rate: 0.1\n"
      ],
      "metadata": {
        "id": "ulwa429987Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "import subprocess\n",
        "import sys\n",
        "import pkg_resources\n",
        "\n",
        "packages = [\n",
        "    'pyyaml',\n",
        "    'pandas',\n",
        "    'torch',\n",
        "    'transformers',\n",
        "    'datasets',\n",
        "    'scikit-learn',\n",
        "    'huggingface-hub',\n",
        "    'optuna',\n",
        "    'numpy'\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        # Attempt to install the package\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "        # After installation, verify if the package is installed\n",
        "        dist = pkg_resources.get_distribution(package)\n",
        "        print(f\"Successfully installed {package} with version {dist.version}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Failed to install {package}. Error: {str(e)}\")\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        print(f\"{package} was not found after installation attempt. Please check for errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPn76xvTI9wb",
        "outputId": "72600c24-a9bf-48ed-b1b4-fc2b0bac76f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed pyyaml with version 6.0.1\n",
            "Successfully installed pandas with version 1.5.3\n",
            "Successfully installed torch with version 2.1.0+cu121\n",
            "Successfully installed transformers with version 4.35.2\n",
            "Successfully installed datasets with version 2.16.1\n",
            "Successfully installed scikit-learn with version 1.2.2\n",
            "Successfully installed huggingface-hub with version 0.20.3\n",
            "Successfully installed optuna with version 3.5.0\n",
            "Successfully installed numpy with version 1.23.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUSC47es8x-D"
      },
      "outputs": [],
      "source": [
        "# Modularized and Dynamic Training Script for Mistral 7B with LoRA, PEFT, and Hyperparameter Optimization using config file\n",
        "\n",
        "import logging\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq, trainer_utils)\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import notebook_login, HfFolder\n",
        "import optuna\n",
        "from optuna.integration import HuggingFacePruner\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "notebook_login()\n",
        "\n",
        "# Setting up logging for debugging and tracking\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 1. Configuration Management\n",
        "class ConfigManager:\n",
        "    def __init__(self, config_path):\n",
        "        with open(config_path, 'r') as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "    def get(self, path, default=None):\n",
        "        keys = path.split('.')\n",
        "        value = self.config\n",
        "        for key in keys:\n",
        "            value = value.get(key, None)\n",
        "            if value is None:\n",
        "                return default\n",
        "        return value\n",
        "\n",
        "    def get_dataset_config(self, dataset_name, config_type):\n",
        "        datasets_info = self.get('datasets_info', [])\n",
        "        for dataset_info in datasets_info:\n",
        "            if dataset_info['dataset_name'] == dataset_name:\n",
        "                return dataset_info.get(config_type, None)\n",
        "        return None\n",
        "\n",
        "# 2. Authentication and Setup\n",
        "class HfAuthenticator:\n",
        "    @staticmethod\n",
        "    def authenticate():\n",
        "        notebook_login()\n",
        "\n",
        "# 3. Model and Tokenizer Initialization\n",
        "class ModelInitializer:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def initialize(self):\n",
        "        try:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=torch.bfloat16)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error initializing model and tokenizer: {e}\")\n",
        "            raise\n",
        "\n",
        "# 4. LoRA Configuration\n",
        "class LoraConfigurer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def configure(self, lora_config=None):\n",
        "        if lora_config is None:\n",
        "            raise ValueError(\"No LoRA configuration provided.\")\n",
        "        try:\n",
        "            # Preparing model for quantized INT8 training for efficiency\n",
        "            self.model = prepare_model_for_int8_training(self.model)\n",
        "            # Applying PEFT for efficient distributed training\n",
        "            self.model = get_peft_model(self.model, lora_config)  # Use lora_config here\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error configuring LoRA: {e}\")\n",
        "            raise\n",
        "\n",
        "# 5. Dataset Management\n",
        "class DatasetManager:\n",
        "    def __init__(self, dataset_info, tokenizer):\n",
        "        self.dataset_info = dataset_info\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def load_and_prepare(self):\n",
        "        try:\n",
        "            dataset = load_dataset(self.dataset_info['dataset_name'], split=self.dataset_info.get('split', 'train'))\n",
        "            dataset = self.auto_preprocess(dataset)\n",
        "            return dataset\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading and preparing dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def auto_preprocess(self, dataset):\n",
        "        text_fields = self.dataset_info.get('text_fields', [])\n",
        "        label_fields = self.dataset_info.get('label_fields', {})\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            # Process text fields\n",
        "            if text_fields:\n",
        "                concatenated_text = [' '.join([examples[field] for field in text_fields]) for _ in range(len(examples[text_fields[0]]))]\n",
        "                tokenized_examples = self.tokenizer(concatenated_text, padding='max_length', truncation=True, max_length=self.dataset_info.get('max_token_length', 512))\n",
        "            else:\n",
        "                raise ValueError(f\"No text fields provided in the dataset_info for dataset: {self.dataset_info['dataset_name']}.\")\n",
        "\n",
        "            # Process label fields\n",
        "            for label_field, label_type in label_fields.items():\n",
        "                if label_type == 'categorical':\n",
        "                    tokenized_examples['labels'] = examples[label_field]\n",
        "                elif label_type == 'continuous':\n",
        "                    tokenized_examples['labels'] = [[float(label)] for label in examples[label_field]]\n",
        "                elif label_type == 'multi_label':\n",
        "                    tokenized_examples['labels'] = [list(map(float, label.split(','))) for label in examples[label_field]]\n",
        "\n",
        "            return tokenized_examples\n",
        "\n",
        "        dataset = dataset.map(preprocess_function, batched=True)\n",
        "        return dataset\n",
        "\n",
        "# 6. Training Loop and Evaluation\n",
        "class TrainerWrapper:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.trainer = None  # Initialize trainer only in the train method\n",
        "\n",
        "    def train(self, train_dataset, test_dataset, training_args):\n",
        "        if training_args is None:\n",
        "            raise ValueError(\"No training arguments provided.\")\n",
        "        training_args_obj = TrainingArguments(**training_args)\n",
        "        try:\n",
        "            data_collator = DataCollatorForSeq2Seq(self.tokenizer, model=self.model, label_pad_token_id=-100, pad_to_multiple_of=8)\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=training_args_obj,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=test_dataset,\n",
        "                data_collator=data_collator,\n",
        "                callbacks=[CustomCheckpointCallback(self.model, training_args_obj.output_dir)]\n",
        "            )\n",
        "            self.trainer.train()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during model training: {e}\")\n",
        "            raise\n",
        "\n",
        "# 7. Hyperparameter Optimization\n",
        "class HyperparameterOptimizer:\n",
        "    def __init__(self, model, tokenizer, training_args_template, datasets_info):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.training_args_template = training_args_template\n",
        "        self.datasets_info = datasets_info\n",
        "        self.study = None  # Initialized in optimize()\n",
        "\n",
        "    def objective(self, trial, training_args_template, optuna_config):\n",
        "        if optuna_config is None:\n",
        "            raise ValueError(\"No Optuna configuration provided.\")\n",
        "        # Fetch Optuna parameter ranges from the config\n",
        "        learning_rate_low = optuna_config['param_ranges']['learning_rate']['low']\n",
        "        learning_rate_high = optuna_config['param_ranges']['learning_rate']['high']\n",
        "        num_train_epochs_low = optuna_config['param_ranges']['num_train_epochs']['low']\n",
        "        num_train_epochs_high = optuna_config['param_ranges']['num_train_epochs']['high']\n",
        "        per_device_train_batch_size_options = optuna_config['param_ranges']['per_device_train_batch_size']['options']\n",
        "\n",
        "        # Optuna suggests hyperparameters\n",
        "        learning_rate = trial.suggest_loguniform(\"learning_rate\", learning_rate_low, learning_rate_high)\n",
        "        num_train_epochs = trial.suggest_int(\"num_train_epochs\", num_train_epochs_low, num_train_epochs_high)\n",
        "        per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", per_device_train_batch_size_options)\n",
        "\n",
        "        # Update training arguments with suggestions\n",
        "        training_args = TrainingArguments(\n",
        "            **training_args_template,\n",
        "            learning_rate=learning_rate,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "        )\n",
        "\n",
        "        # Train the model with suggested hyperparameters\n",
        "        trainer_wrapper = TrainerWrapper(self.model, training_args)\n",
        "        for dataset_info in self.datasets_info:\n",
        "            dataset_manager = DatasetManager(dataset_info, self.tokenizer)\n",
        "            dataset = dataset_manager.load_and_prepare()\n",
        "            train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)\n",
        "            trainer_wrapper.train(train_dataset, test_dataset)\n",
        "\n",
        "        # Evaluate the model\n",
        "        eval_result = trainer_wrapper.trainer.evaluate()\n",
        "        return eval_result['eval_loss']  # or another metric that you prefer\n",
        "\n",
        "    def optimize(self, training_args_template, optuna_config):\n",
        "      if optuna_config is None:\n",
        "            raise ValueError(\"No Optuna configuration provided.\")\n",
        "        self.study = optuna.create_study(\n",
        "            study_name=optuna_config['study_name'],\n",
        "            direction=optuna_config['direction'],\n",
        "            pruner=HuggingFacePruner()\n",
        "        )\n",
        "        self.study.optimize(\n",
        "            lambda trial: self.objective(trial, training_args_template, optuna_config),\n",
        "            n_trials=optuna_config['n_trials']\n",
        "        )\n",
        "        return self.study.best_trial.params\n",
        "\n",
        "# 8. Logging and Debugging - Already integrated using Python's logging module.\n",
        "\n",
        "# 9. Utility Functions\n",
        "def input_with_validation(prompt, type_=None, validation=None, error_msg='Invalid input'):\n",
        "    \"\"\"Validates and converts user input.\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            value = input(prompt)\n",
        "            if type_:\n",
        "                value = type_(value)\n",
        "            if validation and not validation(value):\n",
        "                raise ValueError\n",
        "            return value\n",
        "        except ValueError:\n",
        "            print(error_msg)\n",
        "\n",
        "def save_model(model, tokenizer, output_dir):\n",
        "    \"\"\"Saves the model and tokenizer to the specified directory.\"\"\"\n",
        "    try:\n",
        "        model.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving model and tokenizer: {e}\")\n",
        "        raise\n",
        "\n",
        "# Custom Callback for Checkpointing during training\n",
        "class CustomCheckpointCallback(trainer_utils.Callback):\n",
        "    def __init__(self, model, output_dir, save_step=100, max_checkpoints=3):\n",
        "        self.model = model\n",
        "        self.output_dir = output_dir\n",
        "        self.save_step = save_step\n",
        "        self.max_checkpoints = max_checkpoints\n",
        "        self.saved_checkpoints = []\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % self.save_step == 0:\n",
        "            checkpoint_dir = os.path.join(self.output_dir, f'checkpoint-{state.global_step}')\n",
        "            self.model.save_pretrained(checkpoint_dir)\n",
        "            self.saved_checkpoints.append(checkpoint_dir)\n",
        "\n",
        "            # Remove older checkpoints\n",
        "            if len(self.saved_checkpoints) > self.max_checkpoints:\n",
        "                oldest_checkpoint = self.saved_checkpoints.pop(0)\n",
        "                if os.path.isdir(oldest_checkpoint):\n",
        "                    shutil.rmtree(oldest_checkpoint)\n",
        "\n",
        "# Experience Replay\n",
        "def experience_replay(previous_dataset, current_dataset, replay_rate=0.1):\n",
        "    \"\"\"\n",
        "    This function takes a fraction of the previous dataset and adds it to the current training dataset.\n",
        "    It helps in preventing catastrophic forgetting by retraining the model on a portion of the previous data.\n",
        "    \"\"\"\n",
        "    replay_samples = previous_dataset.shuffle(seed=42).select(range(int(replay_rate * len(previous_dataset))))\n",
        "    combined_dataset = concatenate_datasets([current_dataset, replay_samples])\n",
        "    return combined_dataset.shuffle(seed=42)\n",
        "\n",
        "# Main Script Execution\n",
        "def main():\n",
        "    try:\n",
        "        # Read configuration from YAML file\n",
        "        config_manager = ConfigManager('config.yaml')\n",
        "        datasets_info = config_manager.get('datasets_info')\n",
        "\n",
        "        # Authenticate with Hugging Face\n",
        "        HfAuthenticator.authenticate()\n",
        "\n",
        "        # Model and tokenizer initialization\n",
        "        model_name = config_manager.get('model.name')\n",
        "        model_initializer = ModelInitializer(model_name)\n",
        "        model_initializer.initialize()\n",
        "\n",
        "        lora_configurer = LoraConfigurer(model_initializer.model)\n",
        "        trainer_wrapper = TrainerWrapper(model_initializer.model, model_initializer.tokenizer)\n",
        "\n",
        "        previous_datasets = []\n",
        "        replay_rate = 0.1  # Define replay rate\n",
        "\n",
        "        for dataset_info in datasets_info:\n",
        "            dataset_name = dataset_info['dataset_name']\n",
        "\n",
        "            # Fetch dataset-specific configs\n",
        "            lora_config = config_manager.get_dataset_config(dataset_name, 'lora.config')\n",
        "            training_args = config_manager.get_dataset_config(dataset_name, 'training.args')\n",
        "            optuna_config = config_manager.get_dataset_config(dataset_name, 'optuna')\n",
        "\n",
        "            # Configure LoRA with dataset-specific settings\n",
        "            lora_configurer.configure(lora_config)\n",
        "\n",
        "            dataset_manager = DatasetManager(dataset_info, model_initializer.tokenizer)\n",
        "            current_dataset = dataset_manager.load_and_prepare()\n",
        "\n",
        "            # If Optuna optimization is enabled for the dataset\n",
        "            if optuna_config and optuna_config['enable']:\n",
        "                # Ensure proper use of HyperparameterOptimizer with correct arguments\n",
        "                hyperparam_optimizer = HyperparameterOptimizer(\n",
        "                    model_initializer.model,\n",
        "                    model_initializer.tokenizer,\n",
        "                    training_args,  # Pass the correct training arguments template\n",
        "                    datasets_info   # Pass the datasets information\n",
        "                )\n",
        "                best_params = hyperparam_optimizer.optimize(training_args, optuna_config)\n",
        "                # Ensure that best_params are correctly integrated into training_args\n",
        "                for param, value in best_params.items():\n",
        "                    training_args[param] = value\n",
        "\n",
        "\n",
        "            # Experience Replay and Training\n",
        "            if previous_datasets:\n",
        "                current_dataset = experience_replay(concatenate_datasets(previous_datasets), current_dataset, replay_rate=replay_rate)\n",
        "            train_dataset, test_dataset = train_test_split(current_dataset, test_size=0.2)\n",
        "            trainer_wrapper.train(train_dataset, test_dataset, training_args)\n",
        "\n",
        "            previous_datasets.append(current_dataset)\n",
        "\n",
        "        # Uploading the trained model to Hugging Face for easy access and version control\n",
        "        save_model(trainer_wrapper.model, model_initializer.tokenizer, training_args['output_dir'])\n",
        "        if trainer_wrapper.trainer.is_world_process_zero():\n",
        "            model_initializer.model.push_to_hub(f\"{model_name}_trained_model\")\n",
        "\n",
        "    } except Exception as e {\n",
        "        logging.error(f\"An error occurred in the main script: {e}\")\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}