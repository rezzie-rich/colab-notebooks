{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvYxUD0CHAZvz7452fP21A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezzie-rich/colab-notebooks/blob/main/coco_se_unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup\n",
        "\n",
        "These commands install Unsloth and other necessary libraries. Unsloth is installed from its GitHub repository, and specific versions of libraries like trl, peft, accelerate, bitsandbytes, and safetensors are installed to ensure compatibility and functionality."
      ],
      "metadata": {
        "id": "izUXSGE_TFPJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckjl5f1iLGRY"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "The essential libraries for model training and fine-tuning are imported. These include Unsloth's FastLanguageModel, datasets for loading data, SFTTrainer from the TRL library for training, and necessary components from the Transformers library."
      ],
      "metadata": {
        "id": "UixPBWLXS-CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template"
      ],
      "metadata": {
        "id": "5e7VDO8cS7gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration\n",
        "\n",
        "Configuration settings for the model. max_seq_length sets the maximum sequence length for inputs. dtype is set to auto-detect the appropriate data type, and load_in_4bit enables QLoRA for efficient 4-bit quantization."
      ],
      "metadata": {
        "id": "GWeAf_stS4G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 131072\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Enable QLoRA"
      ],
      "metadata": {
        "id": "_7g6uxIbS2Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to validate dataset fields\n",
        "\n",
        "This function is crucial for data validation to ensure that the necessary data structure is in place before any training or processing begins. It helps prevent runtime errors that would occur if expected data fields are missing."
      ],
      "metadata": {
        "id": "2NExYthMS0Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dataset(dataset, required_field):\n",
        "    if required_field not in dataset.column_names:\n",
        "        raise ValueError(f\"Required field '{required_field}' not found in the dataset. Available fields: {dataset.column_names}\")"
      ],
      "metadata": {
        "id": "iydXUOfESyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to log error messages\n",
        "\n",
        "This function is used throughout the script in try-except blocks to log specific errors encountered during various stages of execution. It helps in identifying and diagnosing issues efficiently by providing context about where and what kind of error occurred."
      ],
      "metadata": {
        "id": "QHgtHaj2SudR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_error(stage, error):\n",
        "    print(f\"Error during {stage}: {error}\")"
      ],
      "metadata": {
        "id": "6XQmWFiISs0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model and tokenizer\n",
        "\n",
        "The model and tokenizer are initialized from a pre-trained model provided by Unsloth. The settings for maximum sequence length, data type, and 4-bit loading are applied. If an error occurs during initialization, it is logged."
      ],
      "metadata": {
        "id": "9cSqVVIGSo2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit\n",
        "    )\n",
        "except Exception as e:\n",
        "    log_error(\"model initialization\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "8WGS0Y7MSnJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add LoRA adapters for the first dataset\n",
        "\n",
        "LoRA adapters are added to the model. These adapters help in reducing the number of trainable parameters, making the training process more efficient. The parameters such as r, lora_alpha, and lora_dropout configure the LoRA setup."
      ],
      "metadata": {
        "id": "Z11ZAiZhJfFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank of the adaptation\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.4,  # No dropout for LoRA\n",
        "    bias=\"none\",  # No bias adaptation\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use Unsloth's gradient checkpointing\n",
        "    random_state=3399,\n",
        "    use_rslora=True,   # Use rank stabilized LoRA\n",
        "    loftq_config=None  # No additional quantization configuration\n",
        ")"
      ],
      "metadata": {
        "id": "-xvdXFl1JhaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation function\n",
        "\n",
        "A function to format the prompts from the dataset. It applies a chat template to each conversation in the dataset to standardize the input format for training."
      ],
      "metadata": {
        "id": "drYt3vE2ePyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}"
      ],
      "metadata": {
        "id": "XMIM-ZqPeQQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare the first dataset\n",
        "\n",
        "The first dataset is loaded from Hugging Face. The dataset is validated to ensure it contains the required fields. The tokenizer is configured with a chat template, and the dataset is formatted accordingly."
      ],
      "metadata": {
        "id": "UjBgWJ3ZSlNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    dataset1 = load_dataset(\"allenai/dolma\")\n",
        "    validate_dataset(dataset1['train'], \"text\")\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"phi-3\",\n",
        "        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"}\n",
        "    )\n",
        "    dataset1 = dataset1.map(formatting_prompts_func, batched=True)\n",
        "except Exception as e:\n",
        "    log_error(\"loading or validating the first dataset\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "Ku1s6fMjSj5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determine the appropriate dtype for mixed precision training\n",
        "\n",
        "This logic checks if the hardware supports Bfloat16. If not, it defaults to using Float16. This helps in optimizing the training process based on the hardware capabilities."
      ],
      "metadata": {
        "id": "ik4iHxFRgDeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_fp16 = not is_bfloat16_supported()\n",
        "use_bf16 = is_bfloat16_supported()"
      ],
      "metadata": {
        "id": "WpstwlYvgFyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with the first dataset\n",
        "\n",
        "The training arguments are set up, including batch size, learning rate, and the use of mixed precision. SFTTrainer is used to train the model with the first dataset. If any error occurs, it is logged."
      ],
      "metadata": {
        "id": "8_ybYGLwSiPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    training_args1 = TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=16,\n",
        "        warmup_steps=800,\n",
        "        max_steps=300000,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=use_fp16,\n",
        "        bf16=use_bf16,\n",
        "        logging_steps=7,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.07,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3399,\n",
        "        output_dir=\"/home/reza/Documents/workSHOP/unsloth-models/outputs_phase1\",\n",
        "    )\n",
        "\n",
        "    trainer1 = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset1['train'],\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=4,\n",
        "        packing=True,\n",
        "        args=training_args1,\n",
        "    )\n",
        "\n",
        "    trainer1.train()\n",
        "except Exception as e:\n",
        "    log_error(\"training with the first dataset\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "uBMHRi5sSgW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the intermediate model\n",
        "\n",
        "After training with the first dataset, the model is saved."
      ],
      "metadata": {
        "id": "_MkDCLtwSdd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model.save_pretrained(\"/home/reza/Documents/workSHOP/unsloth-models/intermediate_model\")\n",
        "except Exception as e:\n",
        "    log_error(\"saving the intermediate model\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "poxcA2GTScAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the intermediate model\n",
        "\n",
        "Model is then reloaded for further training with the second dataset. This helps in managing the training process and checkpoints effectively."
      ],
      "metadata": {
        "id": "xSErfDkQSaen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    model = FastLanguageModel.from_pretrained(\"/home/reza/Documents/workSHOP/unsloth-models/intermediate_model\")\n",
        "except Exception as e:\n",
        "    log_error(\"loading the intermediate model\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "-y4mMGWMSZF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add LoRA adapters for the second dataset\n",
        "\n",
        "LoRA adapters are added again with different settings for the second dataset. This allows for flexibility in fine-tuning the model with various data characteristics."
      ],
      "metadata": {
        "id": "F3WzRUnkJ3xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(\n",
        "    model,\n",
        "    r=32,  # Different rank for the second dataset\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.2,  # Different dropout for LoRA\n",
        "    bias=\"none\",  # No bias adaptation\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Use Unsloth's gradient checkpointing\n",
        "    random_state=3399,\n",
        "    use_rslora=True,   # Use rank stabilized LoRA\n",
        "    loftq_config=None  # No additional quantization configuration\n",
        ")"
      ],
      "metadata": {
        "id": "aMOXNe_7J4n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare the second dataset\n",
        "\n",
        "The second dataset is loaded and validated. The tokenizer is configured with a chat template, and the dataset is formatted for training."
      ],
      "metadata": {
        "id": "plglUt_KSXgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    dataset2 = load_dataset(\"xingyaoww/code-act\")\n",
        "    validate_dataset(dataset2['train'], \"code\")\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"phi-3\",\n",
        "        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"}\n",
        "    )\n",
        "    dataset2 = dataset2.map(formatting_prompts_func, batched=True)\n",
        "except Exception as e:\n",
        "    log_error(\"loading or validating the second dataset\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "jdbgo65_SVpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with the second dataset\n",
        "\n",
        "Similar to the first dataset, the training arguments are set up, and SFTTrainer is used to train the model with the second dataset."
      ],
      "metadata": {
        "id": "nIb0cUE9STSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    training_args2 = TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=50,\n",
        "        max_steps=1000,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=use_fp16,\n",
        "        bf16=use_bf16,\n",
        "        logging_steps=7,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.07,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3399,\n",
        "        output_dir=\"/home/reza/Documents/workSHOP/unsloth-models/outputs_phase2\",\n",
        "    )\n",
        "\n",
        "    trainer2 = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=dataset2['train'],\n",
        "        dataset_text_field=\"code\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=4,\n",
        "        packing=True,\n",
        "        args=training_args2,\n",
        "    )\n",
        "\n",
        "    trainer2.train()\n",
        "except Exception as e:\n",
        "    log_error(\"training with the second dataset\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "04jegNoBSRS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "The inference is set up to generate text responses using the trained model. The tokenizer is configured with a chat template, and input messages are prepared for generation. TextStreamer is used to generate outputs efficiently."
      ],
      "metadata": {
        "id": "dVRbergeKMEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"phi-3\",\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"}\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate output using TextStreamer\n",
        "streamer = TextStreamer(tokenizer)\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True, streamer=streamer)\n",
        "\n",
        "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(\"Generated response:\", response[0])"
      ],
      "metadata": {
        "id": "OSCZjOG4KR11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the final model in FP16 safetensors format\n",
        "\n",
        "The final model is saved in FP16 safetensors format for efficient storage and deployment. The model is also pushed to the Hugging Face Hub, making it accessible for future use."
      ],
      "metadata": {
        "id": "gMV4IgHxSNIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_name01 = \"coco-se-phi3-medium-pro-128k\"\n",
        "try:\n",
        "    model = model.to(dtype=torch.float16)\n",
        "    model.save_pretrained(\n",
        "        f\"/home/reza/Documents/workSHOP/unsloth-models/{final_model_name01}\",\n",
        "        save_safetensors=True\n",
        "    )\n",
        "    model.push_to_hub(\n",
        "        f\"{final_model_name01}\",\n",
        "        token=\"\"  # Replace with your Hugging Face token\n",
        "    )\n",
        "except Exception as e:\n",
        "    log_error(\"saving FP16 safetensors model\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "hE6jn3rVSKt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize and save Q8 model in safetensors format\n",
        "\n",
        "The final model is then saved in Q8 safetensors format for more efficient storage and deployment. The model is quantized to 8 bits (Q8) to reduce its size and computational requirements while maintaining performance. The model is also pushed to the Hugging Face Hub, making it accessible for future use."
      ],
      "metadata": {
        "id": "k346jWRRSH-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_name02 = \"coco-se-phi3-medium-128k\"\n",
        "try:\n",
        "    quantized_model = model.quantize(\n",
        "        bits=8,\n",
        "        dtype=torch.float16,\n",
        "        quantization_method=\"q8_0\"\n",
        "    )\n",
        "    quantized_model.save_pretrained(\n",
        "        f\"/home/reza/Documents/workSHOP/unsloth-models/{final_model_name02}\",\n",
        "        save_safetensors=True\n",
        "    )\n",
        "    quantized_model.push_to_hub(\n",
        "        f\"{final_model_name02}\",\n",
        "        token=\"\"  # Replace with your Hugging Face token\n",
        "    )\n",
        "except Exception as e:\n",
        "    log_error(\"saving Q8 safetensors model\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "ZvzIn7l8SFVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize and save Q8 model in GGUF format\n",
        "\n",
        "Finally the model is saved in Q8 and GGUF for CPU only usage."
      ],
      "metadata": {
        "id": "mP4RBeCBR9eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_name03 = \"coco-se-phi3-medium-128k-GGUF\"\n",
        "try:\n",
        "    model.save_pretrained_gguf(\n",
        "        f\"/home/reza/Documents/workSHOP/unsloth-models/{final_model_name03}\",\n",
        "        quantization_method=\"q8_0\"\n",
        "    )\n",
        "    model.push_to_hub_gguf(\n",
        "        f\"{final_model_name03}\",\n",
        "        quantization_method=\"q8_0\",\n",
        "        token=\"\"  # Replace with your Hugging Face token\n",
        "    )\n",
        "except Exception as e:\n",
        "    log_error(\"saving or pushing GGUF model\", e)\n",
        "    raise"
      ],
      "metadata": {
        "id": "dmZb7c2qR51y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}