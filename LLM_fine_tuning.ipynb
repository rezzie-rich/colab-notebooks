{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv/1FgepU6Ay9WQjI+k3f4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezzie-rich/colab-notebooks/blob/main/LLM_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Installation:\n",
        "First, install the necessary libraries and authenticate with Hugging Face."
      ],
      "metadata": {
        "id": "ry_Gr8JPZkF3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6oyP85sZJj6"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets bitsandbytes-cuda11 peft huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Base Model and Dataset:\n",
        "Load your base model and dataset from Hugging Face."
      ],
      "metadata": {
        "id": "YaqopSAgZsLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "dataset_name = \"your_dataset_name_here\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dataset = load_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "jHtu7H3_ZyiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure LoRA:\n",
        "Set up PEFT LoRA according to your requirements. Adjust r and alpha values as needed for your LoRA configuration."
      ],
      "metadata": {
        "id": "2B1RtfyfZ0Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "\n",
        "lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"])\n",
        "model = prepare_model_for_kbit_training(model, lora_config)"
      ],
      "metadata": {
        "id": "mOsv0rphZ97s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Control:\n",
        "Define the hyperparameters for training."
      ],
      "metadata": {
        "id": "RugN4o1zaBXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = {\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"per_device_train_batch_size\": 16,\n",
        "    \"num_train_epochs\": 3,\n",
        "    # Add other hyperparameters here\n",
        "}"
      ],
      "metadata": {
        "id": "04MIhp27aJ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:\n",
        "Set up the training procedure. Ensure efficient training with mixed precision and gradient checkpointing for memory efficiency."
      ],
      "metadata": {
        "id": "OvTQ5XG8aOdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model_output\",\n",
        "    **training_args,\n",
        "    fp16=True, # for mixed precision training\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "R85ooAiXaR57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the Model to Hugging Face:\n",
        "After training, upload the model back to Hugging Face."
      ],
      "metadata": {
        "id": "CjbvYmZqaVpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"Your_HF_Repo_Name\")"
      ],
      "metadata": {
        "id": "ZR4uEhBSagXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Notes:\n",
        "\n",
        "Ensure you have enough GPU memory available for training. Mistral 7B is a large model.\n",
        "Adjust the learning rate and other hyperparameters according to your specific needs and dataset.\n",
        "If you face any memory issues, try reducing the batch size or sequence length."
      ],
      "metadata": {
        "id": "zANJlwGQam2R"
      }
    }
  ]
}